{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c10cb0c",
   "metadata": {},
   "source": [
    "''' -- coding:utf-8 -- @author: Huaiyu Dong ＆ Yixing Huang @Time: 2023-10-23 @Explanition: This code aims to predict the EAB of microwave absorber structure using the DNN neural network architecture with GA algorithm. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843daf19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###归一化、无数据增强功能###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set random seed for PyTorch and NumPy\n",
    "#seed = 14618\n",
    "#torch.manual_seed(seed)\n",
    "#np.random.seed(seed)\n",
    "\n",
    "# Load your data from mydata_petals.csv\n",
    "data = pd.read_csv('mydata_petals.csv')\n",
    "\n",
    "# Extract the last column (EAB) as the target variable\n",
    "EAB = data.iloc[:, -1].values\n",
    "\n",
    "# Extract the first 10 columns as input features\n",
    "input = data.iloc[:, :-1].values\n",
    "\n",
    "# Create MinMaxScaler objects for input and EAB\n",
    "scaler_in = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "scaler_out = MinMaxScaler(feature_range=(-0.99, 0.99))\n",
    "\n",
    "# Fit and transform input data, and store the transformation matrix ps_in\n",
    "p_scaled = scaler_in.fit_transform(input)\n",
    "p = torch.FloatTensor(p_scaled.T)\n",
    "ps_in = scaler_in.scale_\n",
    "\n",
    "# Fit and transform EAB data, and store the transformation matrix ts_out\n",
    "t_scaled = scaler_out.fit_transform(EAB.reshape(-1, 1))\n",
    "t = torch.FloatTensor(t_scaled)\n",
    "ts_out = scaler_out.scale_\n",
    "\n",
    "print(\"Scaled Data Shapes:\")\n",
    "print(p_scaled.shape)\n",
    "print(t_scaled.shape)\n",
    "\n",
    "# Define the ratio for training and testing data\n",
    "train_ratio = 0.8  # 80% training data, \n",
    "test_ratio = 0.2   # 20% testing data\n",
    "\n",
    "# Number of samples\n",
    "n_samples = len(EAB)\n",
    "\n",
    "# Create a random permutation of indices for shuffling\n",
    "indices = np.random.permutation(n_samples)\n",
    "\n",
    "# Calculate the sizes of the training and test sets\n",
    "n_train = int(train_ratio * n_samples)\n",
    "n_test = n_samples - n_train\n",
    "\n",
    "# Use the shuffled indices to split the data into training and test sets\n",
    "train_indices = indices[:n_train]\n",
    "test_indices = indices[n_train:]\n",
    "\n",
    "# Split the data into training and test sets based on the shuffled indices\n",
    "X_train, y_train = p_scaled[train_indices], t_scaled[train_indices]\n",
    "X_test, y_test = p_scaled[test_indices], t_scaled[test_indices]\n",
    "\n",
    "print(\"Training Data Shapes:\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(\"Test Data Shapes:\")\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# 打印训练数据前五行\n",
    "print(\"First 5 rows of Training Data:\")\n",
    "print(X_train[:5])\n",
    "print(\"First 5 rows of Training Target (y_train):\")\n",
    "print(y_train[:5])\n",
    "\n",
    "print(\"Test Data Shapes:\")\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# 打印测试数据前五行\n",
    "print(\"First 5 rows of Test Data:\")\n",
    "print(X_test[:5])\n",
    "print(\"First 5 rows of Test Target (y_test):\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0390ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU-version-仅使用PSO完成DNN权重和偏差的初始化更新##\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Define your DNN architecture\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        super(DNN, self).__init__()\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(1, len(dims)):\n",
    "            self.layers.append(nn.Linear(dims[i - 1], dims[i]))\n",
    "            if i < len(dims) - 1:\n",
    "                self.layers.append(nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Define the fitness function for PSO\n",
    "def fitness_function(weights, dnn, criterion, inputs, targets):\n",
    "    # Apply weights to the DNN model\n",
    "    start_idx = 0\n",
    "    for param in dnn.parameters():\n",
    "        param_size = param.numel()\n",
    "        param.data = torch.FloatTensor(weights[start_idx:start_idx+param_size]).view(param.size())\n",
    "        start_idx += param_size\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = dnn(inputs)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Define input dimension and hidden layer dimensions (adjust as needed)\n",
    "input_dim = 10  \n",
    "hidden_dims = [14, 12, 8, 3]  \n",
    "output_dim = 1  \n",
    "\n",
    "# Add the following lines to define 'X_train' and 'y_train' based on your data\n",
    "X_train = X_train  \n",
    "y_train = y_train \n",
    "\n",
    "# Initialize the DNN\n",
    "dnn = DNN(input_dim, output_dim, hidden_dims)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the problem bounds for PSO\n",
    "num_weights = sum(p.numel() for p in dnn.parameters())\n",
    "lb = [-1.0] * num_weights  # Lower bounds for weights\n",
    "ub = [1.0] * num_weights  # Upper bounds for weights\n",
    "\n",
    "# Initialize PSO parameters\n",
    "swarm_size = 30\n",
    "max_iters = 10\n",
    "c1 = 1.5  # Cognitive coefficient\n",
    "c2 = 1.5 # Social coefficient\n",
    "w = 0.3   # Inertia weight\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize the swarm  \n",
    "swarm = np.random.uniform(lb, ub, (swarm_size, num_weights))\n",
    "velocities = np.zeros((swarm_size, num_weights))\n",
    "pbest_positions = swarm.copy()\n",
    "pbest_scores = np.zeros(swarm_size)\n",
    "gbest_position = None\n",
    "gbest_score = float('inf')\n",
    "\n",
    "\n",
    "for i in range(max_iters):\n",
    "    for j in range(swarm_size):  #最小化损失函数\n",
    "        score = fitness_function(swarm[j], dnn, criterion, torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "        if score < pbest_scores[j]:\n",
    "            pbest_scores[j] = score\n",
    "            pbest_positions[j] = swarm[j]\n",
    "        \n",
    "        if score < gbest_score:   #最小化损失函数\n",
    "            gbest_score = score\n",
    "            gbest_position = swarm[j]\n",
    "        \n",
    "    for j in range(swarm_size):\n",
    "        r1 = np.random.rand(num_weights)\n",
    "        r2 = np.random.rand(num_weights)\n",
    "        velocities[j] = w * velocities[j] + c1 * r1 * (pbest_positions[j] - swarm[j]) + c2 * r2 * (gbest_position - swarm[j])\n",
    "        swarm[j] += velocities[j]\n",
    "\n",
    "# Update the DNN with the global best position\n",
    "start_idx = 0\n",
    "for param in dnn.parameters():\n",
    "    param_size = param.numel()\n",
    "    param.data = torch.FloatTensor(gbest_position[start_idx:start_idx+param_size]).view(param.size())\n",
    "    start_idx += param_size\n",
    "\n",
    "# Training with optimized weights\n",
    "num_epochs = 300\n",
    "\n",
    "optimizer = optim.Adam(dnn.parameters(), lr=0.02)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = dnn(torch.FloatTensor(X_train))\n",
    "    loss = criterion(outputs, torch.FloatTensor(y_train))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "# Calculate and print the training duration\n",
    "training_duration = end_time - start_time\n",
    "print(f\"Training duration: {training_duration:.2f} seconds\")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs+1), losses, marker='o', linestyle='-')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d04104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import math\n",
    "with torch.no_grad():\n",
    "    predicted_train = dnn(torch.FloatTensor(X_train))\n",
    "    train_true_values = y_train\n",
    "    r_train = r2_score(train_true_values, predicted_train)\n",
    "    mae_train = mean_absolute_error(train_true_values, predicted_train)   # MAE training set\n",
    "    mse_train = mean_squared_error(train_true_values, predicted_train)    # MSE training set\n",
    "    rmse_train = math.sqrt(mse_train)                               # RMSE training set\n",
    "\n",
    "print(f\"R-squared (r_train) on training set: {r_train:.3f}\")\n",
    "print(f\"Mean Absolute Error (MAE) on training set: {mae_train:.3f}\")\n",
    "print(f\"Mean Squared Error (MSE) on training set: {mse_train:.3f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on training set: {rmse_train:.3f}\")\n",
    "\n",
    "with torch.no_grad():              # Perform regression analysis on the test set (X_test, y_test)\n",
    "    predicted_test = dnn(torch.FloatTensor(X_test))\n",
    "    test_true_values = y_test\n",
    "    r_test = r2_score(test_true_values, predicted_test)\n",
    "    mae_test = mean_absolute_error(test_true_values, predicted_test)   #MAE test set\n",
    "    mse_test = mean_squared_error(test_true_values, predicted_test)    # MSE test set\n",
    "    rmse_test = math.sqrt(mse_test)                               #RMSE test set\n",
    "    predicted_train = dnn(torch.FloatTensor(X_train))\n",
    "\n",
    "print(f\"R-squared (r_test) on test set: {r_test:.3f}\")\n",
    "print(f\"Mean Absolute Error (MAE) on test set: {mae_test:.3f}\")\n",
    "print(f\"Mean Squared Error (MSE) on test set: {mse_test:.3f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on test set: {rmse_test:.3f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "actual_train = y_train\n",
    "actual_test = y_test  # Replace 'y_test' with your test labels\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Scatter plot of actual values and regression line for training set\n",
    "axes[0].scatter(actual_train, actual_train, color='blue', label='Actual', marker='o', s=10)\n",
    "axes[0].scatter(actual_train, predicted_train, color='red', label='Predicted', marker='x', s=10)\n",
    "axes[0].set_xlim(-1.2, 1.2)\n",
    "axes[0].set_ylim(-1.2, 1.2)\n",
    "correlation_train = np.corrcoef(actual_train, predicted_train, rowvar=False)[0, 1]\n",
    "axes[0].set_title(f'Training Set\\nCorrelation: {correlation_train:.3f}')\n",
    "regression_train = LinearRegression()\n",
    "regression_train.fit(actual_train.reshape(-1, 1), predicted_train)\n",
    "regression_line_train = regression_train.predict(actual_train.reshape(-1, 1))\n",
    "axes[0].plot(actual_train, regression_line_train, color='green', linestyle='--', label='Regression Line')\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter plot of actual values and regression line for test set\n",
    "axes[1].scatter(actual_test, actual_test, color='blue', label='Actual', marker='o', s=10)\n",
    "axes[1].scatter(actual_test, predicted_test, color='red', label='Predicted', marker='x', s=10)\n",
    "axes[1].set_xlim(-1.2, 1.2)\n",
    "axes[1].set_ylim(-1.2, 1.2)\n",
    "correlation_test = np.corrcoef(actual_test, predicted_test, rowvar=False)[0, 1]\n",
    "axes[1].set_title(f'Test Set\\nCorrelation: {correlation_test:.3f}')\n",
    "regression_test = LinearRegression()\n",
    "regression_test.fit(actual_test.reshape(-1, 1), predicted_test)\n",
    "regression_line_test = regression_test.predict(actual_test.reshape(-1, 1))\n",
    "axes[1].plot(actual_test, regression_line_test, color='green', linestyle='--', label='Regression Line')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfed8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc06a30f",
   "metadata": {},
   "source": [
    "#GPU-version-仅使用PSO完成DNN权重和偏差的初始化更新##\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define your DNN architecture\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        super(DNN, self).__init__()\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(1, len(dims)):\n",
    "            self.layers.append(nn.Linear(dims[i - 1], dims[i]))\n",
    "            if i < len(dims) - 1:\n",
    "                self.layers.append(nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Define the fitness function for PSO\n",
    "def fitness_function(weights, dnn, criterion, inputs, targets):\n",
    "    # Apply weights to the DNN model\n",
    "    start_idx = 0\n",
    "    for param in dnn.parameters():\n",
    "        param_size = param.numel()\n",
    "        param.data = torch.FloatTensor(weights[start_idx:start_idx+param_size]).view(param.size())\n",
    "        param.data = param.data.to('cuda')  # Move data to GPU\n",
    "        start_idx += param_size\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = dnn(inputs)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Define input dimension and hidden layer dimensions (adjust as needed)\n",
    "input_dim = 10  # Adjust based on your data\n",
    "hidden_dims = [12, 10, 6, 3]  # Adjust based on your desired architecture\n",
    "output_dim = 1  # Adjust based on your data\n",
    "\n",
    "# Add the following lines to define 'X_train' and 'y_train' based on your data\n",
    "X_train = X_train  # Replace with your input data\n",
    "y_train = y_train  # Replace with your output data\n",
    "\n",
    "# Initialize the DNN\n",
    "dnn = DNN(input_dim, output_dim, hidden_dims).to('cuda')\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the problem bounds for PSO\n",
    "num_weights = sum(p.numel() for p in dnn.parameters())\n",
    "lb = [-1.0] * num_weights  # Lower bounds for weights\n",
    "ub = [1.0] * num_weights  # Upper bounds for weights\n",
    "\n",
    "# Initialize PSO parameters\n",
    "swarm_size = 100\n",
    "max_iters = 100\n",
    "c1 = 1.5  # Cognitive coefficient\n",
    "c2 = 1.5  # Social coefficient\n",
    "w = 0.7    # Inertia weight\n",
    "\n",
    "# Initialize the swarm\n",
    "swarm = np.random.uniform(lb, ub, (swarm_size, num_weights))\n",
    "velocities = np.zeros((swarm_size, num_weights))\n",
    "pbest_positions = swarm.copy()\n",
    "pbest_scores = np.zeros(swarm_size)\n",
    "gbest_position = None\n",
    "gbest_score = float('inf')\n",
    "\n",
    "for i in range(max_iters):\n",
    "    for j in range(swarm_size):\n",
    "        score = fitness_function(swarm[j], dnn, criterion, torch.FloatTensor(X_train).to('cuda'), torch.FloatTensor(y_train).to('cuda'))\n",
    "        if score < pbest_scores[j]:\n",
    "            pbest_scores[j] = score\n",
    "            pbest_positions[j] = swarm[j]\n",
    "        \n",
    "        if score < gbest_score:\n",
    "            gbest_score = score\n",
    "            gbest_position = swarm[j]\n",
    "        \n",
    "    for j in range(swarm_size):\n",
    "        r1 = np.random.rand(num_weights)\n",
    "        r2 = np.random.rand(num_weights)\n",
    "        velocities[j] = w * velocities[j] + c1 * r1 * (pbest_positions[j] - swarm[j]) + c2 * r2 * (gbest_position - swarm[j])\n",
    "        swarm[j] += velocities[j]\n",
    "\n",
    "# Update the DNN with the global best position\n",
    "start_idx = 0\n",
    "for param in dnn.parameters():\n",
    "    param_size = param.numel()\n",
    "    param.data = torch.FloatTensor(gbest_position[start_idx:start_idx+param_size]).view(param.size())\n",
    "    start_idx += param_size\n",
    "\n",
    "# Training with optimized weights\n",
    "optimizer = optim.Adam(dnn.parameters(), lr=0.01)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(3000):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = dnn(torch.FloatTensor(X_train).to('cuda'))\n",
    "    loss = criterion(outputs, torch.FloatTensor(y_train).to('cuda'))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure()\n",
    "plt.plot(range(1, 3001), losses, marker='o', linestyle='-')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac650b",
   "metadata": {},
   "source": [
    "#使用PSO算法完成所有训练轮次权重和偏差的更新#\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define your DNN architecture\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        super(DNN, self).__init__()\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(1, len(dims)):\n",
    "            self.layers.append(nn.Linear(dims[i - 1], dims[i]))\n",
    "            if i < len(dims) - 1:\n",
    "                self.layers.append(nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Define the fitness function for PSO\n",
    "def fitness_function(weights, dnn, criterion, inputs, targets):\n",
    "    # Apply weights to the DNN model\n",
    "    start_idx = 0\n",
    "    for param in dnn.parameters():\n",
    "        param_size = param.numel()\n",
    "        param.data = torch.FloatTensor(weights[start_idx:start_idx+param_size]).view(param.size())\n",
    "        start_idx += param_size\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = dnn(inputs)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Define input dimension and hidden layer dimensions (adjust as needed)\n",
    "input_dim = 10  # Adjust based on your data\n",
    "hidden_dims = [12, 10, 8, 6]  # Adjust based on your desired architecture\n",
    "output_dim = 1  # Adjust based on your data\n",
    "\n",
    "# Add the following lines to define 'X_train' and 'y_train' based on your data\n",
    "X_train = X_train  # Replace with your input data\n",
    "y_train = y_train  # Replace with your output data\n",
    "\n",
    "# Initialize the DNN\n",
    "dnn = DNN(input_dim, output_dim, hidden_dims)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the problem bounds for PSO\n",
    "num_weights = sum(p.numel() for p in dnn.parameters())\n",
    "lb = [-1.0] * num_weights  # Lower bounds for weights\n",
    "ub = [1.0] * num_weights  # Upper bounds for weights\n",
    "\n",
    "# Initialize PSO parameters\n",
    "swarm_size = 50\n",
    "max_iters = 100\n",
    "c1 = 1.5  # Cognitive coefficient\n",
    "c2 = 1.5  # Social coefficient\n",
    "w = 0.7    # Inertia weight\n",
    "\n",
    "# Training with optimized weights\n",
    "optimizer = optim.Adam(dnn.parameters(), lr=0.01)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(max_iters):\n",
    "    # PSO optimization\n",
    "    swarm = np.random.uniform(lb, ub, (swarm_size, num_weights))\n",
    "    velocities = np.zeros((swarm_size, num_weights))\n",
    "    pbest_positions = swarm.copy()\n",
    "    pbest_scores = np.zeros(swarm_size)\n",
    "    gbest_position = None\n",
    "    gbest_score = float('inf')\n",
    "\n",
    "    for j in range(1000):  # Inner training loop\n",
    "        for k in range(swarm_size):\n",
    "            score = fitness_function(swarm[k], dnn, criterion, torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "            if score < pbest_scores[k]:\n",
    "                pbest_scores[k] = score\n",
    "                pbest_positions[k] = swarm[k]\n",
    "\n",
    "            if score < gbest_score:\n",
    "                gbest_score = score\n",
    "                gbest_position = swarm[k]\n",
    "\n",
    "        for k in range(swarm_size):\n",
    "            r1 = np.random.rand(num_weights)\n",
    "            r2 = np.random.rand(num_weights)\n",
    "            velocities[k] = w * velocities[k] + c1 * r1 * (pbest_positions[k] - swarm[k]) + c2 * r2 * (gbest_position - swarm[k])\n",
    "            swarm[k] += velocities[k]\n",
    "\n",
    "        # Update the DNN with the global best position\n",
    "        start_idx = 0\n",
    "        for param in dnn.parameters():\n",
    "            param_size = param.numel()\n",
    "            param.data = torch.FloatTensor(gbest_position[start_idx:start_idx+param_size]).view(param.size())\n",
    "            start_idx += param_size\n",
    "\n",
    "        # Calculate and store loss\n",
    "        optimizer.zero_grad()\n",
    "        outputs = dnn(torch.FloatTensor(X_train))\n",
    "        loss = criterion(outputs, torch.FloatTensor(y_train))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(losses) + 1), losses, marker='o', linestyle='-')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
