{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098785e2",
   "metadata": {},
   "source": [
    "''' -- coding:utf-8 -- @author: Huaiyu Dong ＆ Yixing Huang @Time: 2023-10-24 @Explanition: This code aims to predict the EAB of microwave absorber structure using the Radnom forest model. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple Random forest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#seed = 45\n",
    "# Load your data from mydata_petals.csv\n",
    "data = pd.read_csv('mydata_petals.csv')\n",
    "\n",
    "# Extract the last column (EAB) as the target variable\n",
    "EAB = data.iloc[:, -1].values\n",
    "\n",
    "# Extract the first 10 columns as input features\n",
    "input = data.iloc[:, :-1].values\n",
    "\n",
    "# Create MinMaxScaler objects for input and EAB\n",
    "scaler_in = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "scaler_out = MinMaxScaler(feature_range=(-0.99, 0.99))\n",
    "\n",
    "# Fit and transform input data\n",
    "X = scaler_in.fit_transform(input)\n",
    "y = scaler_out.fit_transform(EAB.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #random_state=seed\n",
    "# 记录开始时间\n",
    "start_time = time.time()\n",
    "# Initialize and train the Random Forest model with specified hyperparameters\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,  # Number of trees in the forest\n",
    "    max_depth=10,  # Maximum depth of each tree (adjust as needed)\n",
    "    min_samples_split=2,  # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=1  # Minimum number of samples required to be at a leaf node\n",
    ") #    ,random_state=seed\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "# 记录结束时间\n",
    "end_time = time.time()\n",
    "# 计算执行时间\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Code execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Predict with the trained model\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Transform the predictions back to the original scale\n",
    "y_train_pred = scaler_out.inverse_transform(y_train_pred.reshape(-1, 1)).ravel()\n",
    "y_test_pred = scaler_out.inverse_transform(y_test_pred.reshape(-1, 1)).ravel()\n",
    "y_train_true = scaler_out.inverse_transform(y_train.reshape(-1, 1)).ravel()\n",
    "y_test_true = scaler_out.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Evaluate the model\n",
    "train_r2 = r2_score(y_train_true, y_train_pred)\n",
    "test_r2 = r2_score(y_test_true, y_test_pred)\n",
    "train_mse = mean_squared_error(y_train_true, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test_true, y_test_pred)\n",
    "\n",
    "print(f\"Training R-squared: {train_r2:.3f}\")\n",
    "print(f\"Testing R-squared: {test_r2:.3f}\")\n",
    "print(f\"Training Mean Squared Error: {train_mse:.3f}\")\n",
    "print(f\"Testing Mean Squared Error: {test_mse:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train_true, y_train_pred, label='Training Data')\n",
    "plt.scatter(y_test_true, y_test_pred, label='Testing Data')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.legend()\n",
    "plt.title('True vs. Predicted Values')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train_true, label='Training Data')\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test_true, label='Testing Data')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb55e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional code to plot correlation\n",
    "actual_train = y_train_true\n",
    "predicted_train = y_train_pred\n",
    "actual_test = y_test_true\n",
    "predicted_test = y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Scatter plot of actual values and regression line for training set\n",
    "axes[0].scatter(actual_train, actual_train, color='blue', label='Actual', marker='o', s=10)\n",
    "axes[0].scatter(actual_train, predicted_train, color='red', label='Predicted', marker='x', s=10)\n",
    "#axes[0].set_xlim(-1.2, 1.2)\n",
    "#axes[0].set_ylim(-1.2, 1.2)\n",
    "correlation_train = np.corrcoef(actual_train, predicted_train, rowvar=False)[0, 1]\n",
    "axes[0].set_title(f'Training Set\\nCorrelation: {correlation_train:.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "# Scatter plot of actual values and regression line for test set\n",
    "axes[1].scatter(actual_test, actual_test, color='blue', label='Actual', marker='o', s=10)\n",
    "axes[1].scatter(actual_test, predicted_test, color='red', label='Predicted', marker='x', s=10)\n",
    "#axes[1].set_xlim(-1.2, 1.2)\n",
    "#axes[1].set_ylim(-1.2, 1.2)\n",
    "correlation_test = np.corrcoef(actual_test, predicted_test, rowvar=False)[0, 1]\n",
    "axes[1].set_title(f'Test Set\\nCorrelation: {correlation_test:.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901e969",
   "metadata": {},
   "source": [
    "#集成AdaBoost的随机森林\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 45\n",
    "\n",
    "# Load your data from mydata_petals.csv\n",
    "data = pd.read_csv('mydata_petals.csv')\n",
    "\n",
    "# Extract the last column (EAB) as the target variable\n",
    "EAB = data.iloc[:, -1].values\n",
    "\n",
    "# Extract the first 10 columns as input features\n",
    "input = data.iloc[:, :-1].values\n",
    "\n",
    "# Create MinMaxScaler objects for input and EAB\n",
    "scaler_in = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "scaler_out = MinMaxScaler(feature_range=(-0.99, 0.99))\n",
    "\n",
    "# Fit and transform input data\n",
    "X = scaler_in.fit_transform(input)\n",
    "y = scaler_out.fit_transform(EAB.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Initialize and train the AdaBoost model with Random Forest as base estimator\n",
    "base_rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "ada_model = AdaBoostRegressor(\n",
    "    base_rf_model,\n",
    "    n_estimators=50,  # Number of boosting stages\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict with the trained model\n",
    "y_train_pred = ada_model.predict(X_train)\n",
    "y_test_pred = ada_model.predict(X_test)\n",
    "\n",
    "# Transform the predictions back to the original scale\n",
    "y_train_pred = scaler_out.inverse_transform(y_train_pred.reshape(-1, 1)).ravel()\n",
    "y_test_pred = scaler_out.inverse_transform(y_test_pred.reshape(-1, 1)).ravel()\n",
    "y_train_true = scaler_out.inverse_transform(y_train.reshape(-1, 1)).ravel()\n",
    "y_test_true = scaler_out.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Evaluate the model\n",
    "train_r2 = r2_score(y_train_true, y_train_pred)\n",
    "test_r2 = r2_score(y_test_true, y_test_pred)\n",
    "train_mse = mean_squared_error(y_train_true, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test_true, y_test_pred)\n",
    "\n",
    "print(f\"Training R-squared: {train_r2:.4f}\")\n",
    "print(f\"Testing R-squared: {test_r2:.4f}\")\n",
    "print(f\"Training Mean Squared Error: {train_mse:.4f}\")\n",
    "print(f\"Testing Mean Squared Error: {test_mse:.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train_true, y_train_pred, label='Training Data')\n",
    "plt.scatter(y_test_true, y_test_pred, label='Testing Data')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.legend()\n",
    "plt.title('True vs. Predicted Values')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train_true, label='Training Data')\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test_true, label='Testing Data')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ccdbc",
   "metadata": {},
   "source": [
    "#集成梯度提升（Gradient Boosting）的random forest model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 45\n",
    "\n",
    "# Load your data from mydata_petals.csv\n",
    "data = pd.read_csv('mydata_petals.csv')\n",
    "\n",
    "# Extract the last column (EAB) as the target variable\n",
    "EAB = data.iloc[:, -1].values\n",
    "\n",
    "# Extract the first 10 columns as input features\n",
    "input = data.iloc[:, :-1].values\n",
    "\n",
    "# Create MinMaxScaler objects for input and EAB\n",
    "scaler_in = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "scaler_out = MinMaxScaler(feature_range=(-0.99, 0.99))\n",
    "\n",
    "# Fit and transform input data\n",
    "X = scaler_in.fit_transform(input)\n",
    "y = scaler_out.fit_transform(EAB.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Initialize and train the Random Forest model with specified hyperparameters\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,  # Number of trees in the forest\n",
    "    max_depth=10,  # Maximum depth of each tree (adjust as needed)\n",
    "    min_samples_split=2,  # Minimum number of samples required to split an internal node\n",
    "    min_samples_leaf=1,  # Minimum number of samples required to be at a leaf node\n",
    "    random_state=seed\n",
    ") \n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict with the trained Random Forest model\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Initialize and train the Gradient Boosting model with specified hyperparameters\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,  # Number of boosting stages (trees)\n",
    "    learning_rate=0.1,  # Step size shrinking to prevent overfitting\n",
    "    max_depth=4,  # Maximum depth of each tree (adjust as needed)\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict with the trained Gradient Boosting model\n",
    "y_train_pred_gb = gb_model.predict(X_train)\n",
    "y_test_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Transform the predictions back to the original scale\n",
    "y_train_pred_rf = scaler_out.inverse_transform(y_train_pred_rf.reshape(-1, 1)).ravel()\n",
    "y_test_pred_rf = scaler_out.inverse_transform(y_test_pred_rf.reshape(-1, 1)).ravel()\n",
    "y_train_pred_gb = scaler_out.inverse_transform(y_train_pred_gb.reshape(-1, 1)).ravel()\n",
    "y_test_pred_gb = scaler_out.inverse_transform(y_test_pred_gb.reshape(-1, 1)).ravel()\n",
    "y_train_true = scaler_out.inverse_transform(y_train.reshape(-1, 1)).ravel()\n",
    "y_test_true = scaler_out.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "train_r2_rf = r2_score(y_train_true, y_train_pred_rf)\n",
    "test_r2_rf = r2_score(y_test_true, y_test_pred_rf)\n",
    "train_mse_rf = mean_squared_error(y_train_true, y_train_pred_rf)\n",
    "test_mse_rf = mean_squared_error(y_test_true, y_test_pred_rf)\n",
    "\n",
    "# Evaluate the Gradient Boosting model\n",
    "train_r2_gb = r2_score(y_train_true, y_train_pred_gb)\n",
    "test_r2_gb = r2_score(y_test_true, y_test_pred_gb)\n",
    "train_mse_gb = mean_squared_error(y_train_true, y_train_pred_gb)\n",
    "test_mse_gb = mean_squared_error(y_test_true, y_test_pred_gb)\n",
    "\n",
    "print(\"Random Forest Model:\")\n",
    "print(f\"Training R-squared: {train_r2_rf:.4f}\")\n",
    "print(f\"Testing R-squared: {test_r2_rf:.4f}\")\n",
    "print(f\"Training Mean Squared Error: {train_mse_rf:.4f}\")\n",
    "print(f\"Testing Mean Squared Error: {test_mse_rf:.4f}\")\n",
    "\n",
    "print(\"Gradient Boosting Model:\")\n",
    "print(f\"Training R-squared: {train_r2_gb:.4f}\")\n",
    "print(f\"Testing R-squared: {test_r2_gb:.4f}\")\n",
    "print(f\"Training Mean Squared Error: {train_mse_gb:.4f}\")\n",
    "print(f\"Testing Mean Squared Error: {test_mse_gb:.4f}\")\n",
    "\n",
    "# Plot the results for Random Forest\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train_true, y_train_pred_rf, label='Training Data (RF)')\n",
    "plt.scatter(y_test_true, y_test_pred_rf, label='Testing Data (RF)')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.legend()\n",
    "plt.title('True vs. Predicted Values (RF)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_train_pred_rf, y_train_pred_rf - y_train_true, label='Training Data (RF)')\n",
    "plt.scatter(y_test_pred_rf, y_test_pred_rf - y_test_true, label='Testing Data (RF)')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Residual Plot (RF)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the results for Gradient Boosting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train_true, y_train_pred_gb, label='Training Data (GB)')\n",
    "plt.scatter(y_test_true, y_test_pred_gb, label='Testing Data (GB)')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.legend()\n",
    "plt.title('True vs. Predicted Values (GB)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_train_pred_gb, y_train_pred_gb - y_train_true, label='Training Data (GB)')\n",
    "plt.scatter(y_test_pred_gb, y_test_pred_gb - y_test_true, label='Testing Data (GB)')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Residual Plot (GB)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621ea2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
