{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a57812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data Shapes:\n",
      "(2000, 10)\n",
      "(2000, 1)\n",
      "Training Data Shapes:\n",
      "(1600, 10) (1600, 1)\n",
      "Test Data Shapes:\n",
      "(400, 10) (400, 1)\n",
      "First 5 rows of Training Data:\n",
      "[[ 0.38088088  0.20570571 -0.07357357 -0.32865731 -0.29468116  0.302\n",
      "   0.37907376  0.17089453  0.37363897 -0.4012016 ]\n",
      " [ 0.37887888  0.34084084  0.02352352  0.21342685 -0.41792965  0.17933333\n",
      "   0.2058319   0.36782377 -0.40773639 -0.02136182]\n",
      " [-0.35185185 -0.12662663  0.48698699  0.37274549  0.26694309  0.242\n",
      "  -0.13150372  0.2329773   0.21977077  0.08210948]\n",
      " [ 0.26176176 -0.17167167  0.45695696  0.29158317  0.00757792 -0.46466667\n",
      "   0.39736993 -0.4506008   0.16475645 -0.25367156]\n",
      " [-0.36586587 -0.16566567  0.23473473  0.0511022   0.00729196  0.03066667\n",
      "   0.43110349 -0.11014686 -0.17421203 -0.3164219 ]]\n",
      "First 5 rows of Training Target (y_train):\n",
      "[[-0.1889313 ]\n",
      " [-0.2040458 ]\n",
      " [ 0.32496183]\n",
      " [ 0.03778626]\n",
      " [-0.00755725]]\n",
      "Test Data Shapes:\n",
      "(400, 10) (400, 1)\n",
      "First 5 rows of Test Data:\n",
      "[[ 0.36586587  0.36086086  0.0035035   0.21042084 -0.3844724  -0.492\n",
      "   0.43081761 -0.42857143 -0.18538682  0.38384513]\n",
      " [ 0.18968969 -0.2007007  -0.18268268 -0.23747495 -0.19430941 -0.10333333\n",
      "  -0.1329331   0.01134846 -0.43638968 -0.01268358]\n",
      " [ 0.22072072 -0.11561562  0.18168168  0.36072144  0.31612811  0.34666667\n",
      "  -0.08261864 -0.35781041  0.38452722  0.21962617]\n",
      " [-0.30880881  0.42392392 -0.43093093 -0.3997996   0.11252502  0.198\n",
      "   0.25014294  0.1588785  -0.22234957 -0.25166889]\n",
      " [-0.21771772  0.13163163  0.41291291  0.44088176 -0.31927366 -0.33933333\n",
      "   0.08004574 -0.15220294 -0.3243553  -0.35580774]]\n",
      "First 5 rows of Test Target (y_test):\n",
      "[[-0.27961832]\n",
      " [ 0.46099237]\n",
      " [-0.14358779]\n",
      " [ 0.06801527]\n",
      " [ 0.23427481]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set random seed for PyTorch and NumPy\n",
    "seed = 14618\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load your data from mydata_petals.csv\n",
    "data = pd.read_csv('mydata_petals.csv')\n",
    "\n",
    "# Extract the last column (EAB) as the target variable\n",
    "EAB = data.iloc[:, -1].values\n",
    "\n",
    "# Extract the first 10 columns as input features\n",
    "input = data.iloc[:, :-1].values\n",
    "\n",
    "# Create MinMaxScaler objects for input and EAB\n",
    "scaler_in = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "scaler_out = MinMaxScaler(feature_range=(-0.99, 0.99))\n",
    "\n",
    "# Fit and transform input data, and store the transformation matrix ps_in\n",
    "p_scaled = scaler_in.fit_transform(input)\n",
    "p = torch.FloatTensor(p_scaled.T)\n",
    "ps_in = scaler_in.scale_\n",
    "\n",
    "# Fit and transform EAB data, and store the transformation matrix ts_out\n",
    "t_scaled = scaler_out.fit_transform(EAB.reshape(-1, 1))\n",
    "t = torch.FloatTensor(t_scaled)\n",
    "ts_out = scaler_out.scale_\n",
    "\n",
    "print(\"Scaled Data Shapes:\")\n",
    "print(p_scaled.shape)\n",
    "print(t_scaled.shape)\n",
    "\n",
    "# Define the ratio for training and testing data\n",
    "train_ratio = 0.80  # 80% training data, \n",
    "test_ratio = 0.20   # 20% testing data\n",
    "\n",
    "# Number of samples\n",
    "n_samples = len(EAB)\n",
    "\n",
    "# Create a random permutation of indices for shuffling\n",
    "indices = np.random.permutation(n_samples)\n",
    "\n",
    "# Calculate the sizes of the training and test sets\n",
    "n_train = int(train_ratio * n_samples)\n",
    "n_test = n_samples - n_train\n",
    "\n",
    "# Use the shuffled indices to split the data into training and test sets\n",
    "train_indices = indices[:n_train]\n",
    "test_indices = indices[n_train:]\n",
    "\n",
    "# Split the data into training and test sets based on the shuffled indices\n",
    "X_train, y_train = p_scaled[train_indices], t_scaled[train_indices]\n",
    "X_test, y_test = p_scaled[test_indices], t_scaled[test_indices]\n",
    "\n",
    "print(\"Training Data Shapes:\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(\"Test Data Shapes:\")\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# 打印训练数据前五行\n",
    "print(\"First 5 rows of Training Data:\")\n",
    "print(X_train[:5])\n",
    "print(\"First 5 rows of Training Target (y_train):\")\n",
    "print(y_train[:5])\n",
    "\n",
    "print(\"Test Data Shapes:\")\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# 打印测试数据前五行\n",
    "print(\"First 5 rows of Test Data:\")\n",
    "print(X_test[:5])\n",
    "print(\"First 5 rows of Test Target (y_test):\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8199cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  OrderedDict([('hidden_dim_1', 8), ('hidden_dim_2', 16), ('hidden_dim_3', 4), ('hidden_dim_4', 5), ('learning_rate', 0.00628528834895659), ('max_epochs', 2738)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Define the architecture of the DNN with hidden layers\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        super(DNN, self).__init__()\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(1, len(dims)):\n",
    "            self.layers.append(nn.Linear(dims[i - 1], dims[i]))\n",
    "            if i < len(dims) - 1:\n",
    "                self.layers.append(nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Wrapper class to make PyTorch model compatible with scikit-optimize\n",
    "class PyTorchWrapper(BaseEstimator):\n",
    "    def __init__(self, model_class, input_dim, output_dim, hidden_dims, learning_rate=0.01, max_epochs=3000):\n",
    "        self.model_class = model_class\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.model_class(self.input_dim, self.output_dim, self.hidden_dims)\n",
    "        inputs = torch.FloatTensor(X)\n",
    "        targets = torch.FloatTensor(y)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        inputs = torch.FloatTensor(X)\n",
    "        outputs = self.model(inputs)\n",
    "        return outputs.detach().numpy()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        mse = np.mean((predictions - y) ** 2)\n",
    "        return -mse  # Negate for minimization\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'model_class': self.model_class,\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "            'hidden_dims': self.hidden_dims,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_epochs': self.max_epochs,\n",
    "        }\n",
    "\n",
    "# Objective function to be used by scikit-optimize\n",
    "def objective(params):\n",
    "    # Extract hyperparameters\n",
    "    learning_rate = params['learning_rate']\n",
    "    max_epochs = int(params['max_epochs'])\n",
    "\n",
    "    # Create PyTorch wrapper\n",
    "    model_params = {\n",
    "        'input_dim': 10,\n",
    "        'output_dim': 1,\n",
    "        'hidden_dims': [\n",
    "            int(params['hidden_dim_1']),\n",
    "            int(params['hidden_dim_2']),\n",
    "            int(params['hidden_dim_3']),\n",
    "            int(params['hidden_dim_4'])\n",
    "        ],\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_epochs': max_epochs\n",
    "    }\n",
    "    model = PyTorchWrapper(DNN, **model_params)\n",
    "\n",
    "    # Calculate performance metric (negative mean squared error for minimization)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error')\n",
    "    avg_score = -np.mean(scores)\n",
    "\n",
    "    return avg_score\n",
    "\n",
    "# Define search space\n",
    "search_spaces = {\n",
    "    'hidden_dim_1': (8, 16),\n",
    "    'hidden_dim_2': (8, 16),\n",
    "    'hidden_dim_3': (4, 10),\n",
    "    'hidden_dim_4': (2, 8),\n",
    "    'learning_rate': (1e-4, 1e-1, 'log-uniform'),\n",
    "    'max_epochs': (100, 3000),\n",
    "}\n",
    "\n",
    "# Create BayesSearchCV object\n",
    "opt = BayesSearchCV(\n",
    "    PyTorchWrapper(model_class=DNN, input_dim=10, output_dim=1, hidden_dims=[8, 8, 4, 2]),\n",
    "    search_spaces,\n",
    "    n_iter=30,\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run optimization process\n",
    "opt.fit(np.array(X_train), np.array(y_train))\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best parameters found: \", opt.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
